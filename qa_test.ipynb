{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import LlamaCppEmbeddings, OpenAIEmbeddings\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://news.ltn.com.tw/news/world/breakingnews/4382800\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 512, chunk_overlap = 10,length_function = len)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaCppEmbeddings 目前有BUG造成運行速度緩慢，先用OpenAIEmbeddings代替 https://github.com/langchain-ai/langchain/pull/5066\n",
    "model_path = \"/home/sung/llm/chinese-alpaca-2-7b/gml-model-q4_0.bin\"\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=LlamaCppEmbeddings(model_path=model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/sung/llm/chinese-alpaca-2-7b/gml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  =  423.59 MB (+ 1024.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 384 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 5061 MB\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llama = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=40,\n",
    "            n_batch=8,\n",
    "            verbose=True,\n",
    "            n_ctx = 2048\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'LK-99 為什麼只能提供理論上的支持',\n",
       " 'result': '根據報導，LK-99的合成相對困難，因為只有很小一部分晶體能夠將銅原子放置在正確的位置，形成超導通道。這意味著LK-99的合成需要巧妙且奇蹟般的排列，並且只有在銅原子滲透到晶格中不太可能的位置時才能實現。因此，LK-99的合成和重現性相對困難，目前只能在理論上提供支持。'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "question = \"LK-99 為什麼只能提供理論上的支持\"\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   294.75 ms\n",
      "llama_print_timings:      sample time =   302.41 ms /   235 runs   (    1.29 ms per token,   777.09 tokens per second)\n",
      "llama_print_timings: prompt eval time = 47905.64 ms /  1467 tokens (   32.66 ms per token,    30.62 tokens per second)\n",
      "llama_print_timings:        eval time =  6704.57 ms /   234 runs   (   28.65 ms per token,    34.90 tokens per second)\n",
      "llama_print_timings:       total time = 56917.57 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'LK-99 為什麼只能提供理論上的支持',\n",
       " 'result': ' \\n首先，LK-99是一種新的材料結構，其基於銅磷灰石（Lead phosphate）的超導體材質。它是由南韓科學家開發出來的新型態超導體材料，並且在過去幾個月中引起了全球科技圈的關注和討論。\\n然而，雖然有學者宣稱成功合成了室溫常壓下的LK-99超導體，但目前尚未有其他研究團體能夠證實其實際存在性或進行到實驗階段。根據南韓科學家所發表的論文，他們提出了一個新的材料結構並使用模擬方法來推測和推算該材料的特性。然而，由於實際合成LK-99超導體仍需完成其他步驟，因此目前僅能提供理論上的支持而不是實證的支持。\\n所以，雖然有學者宣稱成功合成了室溫常壓下的LK-99超導體，但目前尚未有其他研究團體能夠證實其實際存在性或進行到實驗階段。'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llama,retriever=vectorstore.as_retriever())\n",
    "question = \"LK-99 為什麼只能提供理論上的支持\"\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   294.75 ms\n",
      "llama_print_timings:      sample time =   172.72 ms /   150 runs   (    1.15 ms per token,   868.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45715.47 ms /  1421 tokens (   32.17 ms per token,    31.08 tokens per second)\n",
      "llama_print_timings:        eval time =  4022.69 ms /   149 runs   (   27.00 ms per token,    37.04 tokens per second)\n",
      "llama_print_timings:       total time = 51252.23 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'這篇文章中提到，若要合成出真正的「室溫超導體 LK-99」，還需要找到一個特定的晶格排列。由於只有少數晶格可以恰當地將銅放置在正確的位置，因此LBNL的研究員認為該材料很難合成重現。\\n\\n這說明了雖然研究者成功推測出了室溫超導體 LK-99 理論上可能存在的情況，但實際上要找到一個確切的晶格排列還是需要進一步實驗和探究的。因此，LBNL的研究員認為該材料仍然需要更多的科學證明才能被廣泛應用於各種領域中。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template =  \"\"\"\n",
    "使用以下上下文來回答最後的問題。\n",
    "如果你不知道答案，就說你不知道，不要試圖編造答案。\n",
    "最多使用三個句子，並儘可能保持答案簡潔。\n",
    "{context}\n",
    "問題：{question}\n",
    "有用的答案：\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llama,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LK-99只能提供理論上的支持，因為它的合成和排列需要非常特殊的條件和位置，並且只有很小一部分晶體能夠實現這種排列。這使得LK-99的合成和重現變得非常困難。\\n謝謝你的詢問！'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
